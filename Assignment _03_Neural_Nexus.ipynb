{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#EN3150 Assignment 03: Simple convolutional neural network to perform classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "7eUK4ap9XmQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1 .CNN for image classification"
      ],
      "metadata": {
        "id": "SjUTjsahX56F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount the google Drive"
      ],
      "metadata": {
        "id": "VRwdhNEOs0-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov3WBfEBvHh2",
        "outputId": "e9d8dfac-ec39-468d-e17f-aa001299acf7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Dataset"
      ],
      "metadata": {
        "id": "vY_oerfG5pTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm  # Progress bar to monitor the copying process\n",
        "\n",
        "# Define the base directory and the new directories for train, validation, and test sets\n",
        "base_dir = '/content/drive/My Drive/EN3150-Assignment-03-CNN/Images/'\n",
        "train_dir = '/content/drive/My Drive/EN3150-Assignment-03-CNN/Images/train'\n",
        "validation_dir = '/content/drive/My Drive/EN3150-Assignment-03-CNN/Images/validation'\n",
        "test_dir = '/content/drive/My Drive/EN3150-Assignment-03-CNN/Images/test'\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Lists to hold image data and labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Load images from each folder (H1, H2, H3, H5, H6)\n",
        "for folder in os.listdir(base_dir):\n",
        "    folder_path = os.path.join(base_dir, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.endswith('.jpg'):\n",
        "                # Construct full image path\n",
        "                image_path = os.path.join(folder_path, filename)\n",
        "                image_paths.append(image_path)\n",
        "                labels.append(folder)  # Assign label based on folder name\n",
        "\n",
        "# Convert labels to a NumPy array\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "39OXoDUkXw4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the Dataset"
      ],
      "metadata": {
        "id": "RKVleIeiYJz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data: 60% train, 20% validation, 20% test\n",
        "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "    image_paths, labels, test_size=0.4, random_state=42, stratify=labels\n",
        ")\n",
        "validation_paths, test_paths, validation_labels, test_labels = train_test_split(\n",
        "    temp_paths, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
        ")\n",
        "\n",
        "# Print the sizes of the splits\n",
        "print(f'Training set: {len(train_paths)} images')\n",
        "print(f'Validation set: {len(validation_paths)} images')\n",
        "print(f'Test set: {len(test_paths)} images')\n",
        "\n",
        "# Function to copy images to the relevant directories (train, validation, test)\n",
        "def copy_images_to_directories(image_paths, labels, split_dir):\n",
        "    # Create subdirectories for each class in the split directory (train, validation, test)\n",
        "    for class_name in np.unique(labels):\n",
        "        os.makedirs(os.path.join(split_dir, class_name), exist_ok=True)\n",
        "\n",
        "    # Use tqdm to show progress while copying files\n",
        "    for image_path, label in tqdm(zip(image_paths, labels), total=len(image_paths)):\n",
        "        # Determine the class directory for the label\n",
        "        class_dir = os.path.join(split_dir, label)\n",
        "        # Copy the image to the appropriate class directory\n",
        "        shutil.copy(image_path, class_dir)\n",
        "\n",
        "# Copy images to the train, validation, and test directories\n",
        "print(\"Copying images to the train directory...\")\n",
        "copy_images_to_directories(train_paths, train_labels, train_dir)\n",
        "\n",
        "print(\"Copying images to the validation directory...\")\n",
        "copy_images_to_directories(validation_paths, validation_labels, validation_dir)\n",
        "\n",
        "print(\"Copying images to the test directory...\")\n",
        "copy_images_to_directories(test_paths, test_labels, test_dir)\n",
        "\n",
        "print(\"Images have been copied to train, validation, and test directories.\")\n"
      ],
      "metadata": {
        "id": "s6kBEVEAW_Uf",
        "outputId": "61f2fdd0-9c81-4ab5-d2b5-ef8322f8dc81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: 5468 images\n",
            "Validation set: 1823 images\n",
            "Test set: 1823 images\n",
            "Copying images to the train directory...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5468/5468 [14:18<00:00,  6.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying images to the validation directory...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1823/1823 [04:19<00:00,  7.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying images to the test directory...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1823/1823 [04:21<00:00,  6.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images have been copied to train, validation, and test directories.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the CNN Model with modifications"
      ],
      "metadata": {
        "id": "LH5GQXquEoyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the CNN architecture\n",
        "def build_cnn(input_shape,x1, m1, x2, m2, x3, x4, m3, d, num_classes):\n",
        "    model = models.Sequential(name=\"Custom_CNN_Model\")\n",
        "\n",
        "    # Use Input layer to define the input shape\n",
        "    model.add(layers.Input(shape=input_shape, name='Input'))\n",
        "\n",
        "    # First Convolutional Layer\n",
        "    model.add(layers.Conv2D(x1, (m1, m1), activation='relu', name='Conv2D_1'))\n",
        "    model.add(layers.MaxPooling2D((2, 2), name='MaxPooling2D_1'))\n",
        "\n",
        "    # Second Convolutional Layer\n",
        "    model.add(layers.Conv2D(x2, (m2, m2), activation='relu', name='Conv2D_2'))\n",
        "    model.add(layers.MaxPooling2D((2, 2), name='MaxPooling2D_2'))\n",
        "\n",
        "    # Third Convolutional Layer\n",
        "    model.add(layers.Conv2D(x4, (m3, m3), activation='relu', name='Conv2D_3'))\n",
        "    model.add(layers.MaxPooling2D((2, 2), name='MaxPooling2D_3'))\n",
        "\n",
        "    # Fourth Convolutional layer\n",
        "    model.add(layers.Conv2D(x4, (m3, m3), activation='relu', name='Conv2D_4'))\n",
        "    model.add(layers.MaxPooling2D((2, 2), name='MaxPooling2D_4'))\n",
        "\n",
        "    # Flatten the output\n",
        "    model.add(layers.Flatten(name='Flatten'))\n",
        "\n",
        "    # Fully Connected Layer\n",
        "    model.add(layers.Dense(x3, activation='relu', name='Dense_1'))\n",
        "    model.add(layers.Dropout(d, name='Dropout_1'))  # Dropout layer to reduce overfitting\n",
        "\n",
        "    # Output Layer\n",
        "    model.add(layers.Dense(num_classes, activation='softmax', name='Output'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build and compile the CNN model\n",
        "input_shape = (224,224, 3)\n",
        "cnn_model = build_cnn(input_shape,x1=32, m1=3, x2=64, m2=3, x3=128, x4=64, m3=3, d=0.5, num_classes=5)\n",
        "\n",
        "# Custom learning rate\n",
        "custom_lr_optimizer = Adam(learning_rate=0.0001)\n",
        "\n",
        "# Using the optimizer in the model\n",
        "cnn_model.compile(optimizer=custom_lr_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "cnn_model.summary()"
      ],
      "metadata": {
        "id": "3zUPDyzSDPTd",
        "outputId": "2da5f3d6-c1f3-4c9a-ac37-7660540329ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Custom_CNN_Model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Conv2D_1 (Conv2D)           (None, 222, 222, 32)      896       \n",
            "                                                                 \n",
            " MaxPooling2D_1 (MaxPooling  (None, 111, 111, 32)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " Conv2D_2 (Conv2D)           (None, 109, 109, 64)      18496     \n",
            "                                                                 \n",
            " MaxPooling2D_2 (MaxPooling  (None, 54, 54, 64)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " Conv2D_3 (Conv2D)           (None, 52, 52, 64)        36928     \n",
            "                                                                 \n",
            " MaxPooling2D_3 (MaxPooling  (None, 26, 26, 64)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " Conv2D_4 (Conv2D)           (None, 24, 24, 64)        36928     \n",
            "                                                                 \n",
            " MaxPooling2D_4 (MaxPooling  (None, 12, 12, 64)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " Flatten (Flatten)           (None, 9216)              0         \n",
            "                                                                 \n",
            " Dense_1 (Dense)             (None, 128)               1179776   \n",
            "                                                                 \n",
            " Dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " Output (Dense)              (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1273669 (4.86 MB)\n",
            "Trainable params: 1273669 (4.86 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the Model"
      ],
      "metadata": {
        "id": "rGM6NhsoG2RU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define image size and batch size\n",
        "img_size = (224, 224)  # Resize images to 150x150\n",
        "batch_size = 32\n",
        "\n",
        "# Create ImageDataGenerators for training, validation, and test sets\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)  # Normalize images to [0, 1]\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load data from directories\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',  # Use categorical since labels are one-hot encoded\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False  # No shuffling for validation data\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = cnn_model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,\n",
        "    verbose=1\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "IMljv6VxyHmk",
        "outputId": "2640e22e-8e56-4bbe-9729-f090a2ec8d1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5468 images belonging to 5 classes.\n",
            "Found 1823 images belonging to 5 classes.\n",
            "Epoch 1/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m679s\u001b[0m 4s/step - accuracy: 0.6877 - loss: 0.7751 - val_accuracy: 0.6687 - val_loss: 0.8161\n",
            "Epoch 2/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m668s\u001b[0m 4s/step - accuracy: 0.6944 - loss: 0.7753 - val_accuracy: 0.6769 - val_loss: 0.8197\n",
            "Epoch 3/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m658s\u001b[0m 4s/step - accuracy: 0.6952 - loss: 0.7600 - val_accuracy: 0.6506 - val_loss: 0.8369\n",
            "Epoch 4/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m658s\u001b[0m 4s/step - accuracy: 0.6994 - loss: 0.7594 - val_accuracy: 0.6643 - val_loss: 0.8273\n",
            "Epoch 5/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m659s\u001b[0m 4s/step - accuracy: 0.7096 - loss: 0.7359 - val_accuracy: 0.6665 - val_loss: 0.8095\n",
            "Epoch 6/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m702s\u001b[0m 4s/step - accuracy: 0.6976 - loss: 0.7300 - val_accuracy: 0.6742 - val_loss: 0.8134\n",
            "Epoch 7/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m664s\u001b[0m 4s/step - accuracy: 0.7154 - loss: 0.7048 - val_accuracy: 0.6577 - val_loss: 0.8388\n",
            "Epoch 8/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m662s\u001b[0m 4s/step - accuracy: 0.7193 - loss: 0.7089 - val_accuracy: 0.6615 - val_loss: 0.8251\n",
            "Epoch 9/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m679s\u001b[0m 4s/step - accuracy: 0.7147 - loss: 0.7090 - val_accuracy: 0.6714 - val_loss: 0.8199\n",
            "Epoch 10/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m684s\u001b[0m 4s/step - accuracy: 0.7392 - loss: 0.6652 - val_accuracy: 0.6659 - val_loss: 0.8288\n",
            "Epoch 11/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m679s\u001b[0m 4s/step - accuracy: 0.7469 - loss: 0.6529 - val_accuracy: 0.6588 - val_loss: 0.8333\n",
            "Epoch 12/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m659s\u001b[0m 4s/step - accuracy: 0.7390 - loss: 0.6673 - val_accuracy: 0.6550 - val_loss: 0.8614\n",
            "Epoch 13/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 4s/step - accuracy: 0.7406 - loss: 0.6555 - val_accuracy: 0.6720 - val_loss: 0.8493\n",
            "Epoch 14/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m674s\u001b[0m 4s/step - accuracy: 0.7474 - loss: 0.6319 - val_accuracy: 0.6434 - val_loss: 0.9067\n",
            "Epoch 15/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m673s\u001b[0m 4s/step - accuracy: 0.7454 - loss: 0.6378 - val_accuracy: 0.6698 - val_loss: 0.8422\n",
            "Epoch 16/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 4s/step - accuracy: 0.7682 - loss: 0.5949 - val_accuracy: 0.6725 - val_loss: 0.8512\n",
            "Epoch 17/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m667s\u001b[0m 4s/step - accuracy: 0.7671 - loss: 0.5768 - val_accuracy: 0.6714 - val_loss: 0.8569\n",
            "Epoch 18/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m679s\u001b[0m 4s/step - accuracy: 0.7692 - loss: 0.5813 - val_accuracy: 0.6489 - val_loss: 0.8997\n",
            "Epoch 19/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m666s\u001b[0m 4s/step - accuracy: 0.7717 - loss: 0.5914 - val_accuracy: 0.6835 - val_loss: 0.8774\n",
            "Epoch 20/20\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m690s\u001b[0m 4s/step - accuracy: 0.7861 - loss: 0.5384 - val_accuracy: 0.6692 - val_loss: 0.8904\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The `save_format` argument is deprecated in Keras 3. Please remove this argument and pass a file path with either `.keras` or `.h5` extension.Received: save_format=tf",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-162b08451835>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/models/cnn_model_1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, zipped, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m             )\n\u001b[1;32m     68\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0;34m\"The `save_format` argument is deprecated in Keras 3. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;34m\"Please remove this argument and pass a file path with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The `save_format` argument is deprecated in Keras 3. Please remove this argument and pass a file path with either `.keras` or `.h5` extension.Received: save_format=tf"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/drive/My Drive/models/cnn_model_1.h5'\n",
        "cnn_model.save(save_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "wYHoldf0V8UH",
        "outputId": "44be7023-eaf3-4ce8-ccaf-92f20fecb337",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model"
      ],
      "metadata": {
        "id": "CAsgZFTymoc6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aJGHDywUV70f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a data generator for the test dataset\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load test data\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False  # No shuffling to maintain order for evaluation\n",
        ")"
      ],
      "metadata": {
        "id": "p3uILs-ulivg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy = cnn_model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "QftjRy1nmv8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get true labels and predictions\n",
        "true_labels = test_generator.classes\n",
        "predictions = cnn_model.predict(test_generator, verbose=1)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Get class labels\n",
        "class_labels = list(test_generator.class_indices.keys())\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wRiqaP-LnBks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "report = classification_report(true_labels, predicted_labels, target_names=class_labels)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "TPVNGTrjnZdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracy = history.history['accuracy'][-1]\n",
        "val_accuracy = history.history['val_accuracy'][-1]\n",
        "\n",
        "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "jWzDVeMynhe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training and validation loss"
      ],
      "metadata": {
        "id": "seuQ6nZ8nnqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Learning rates to experiment with\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
        "\n",
        "# Dictionary to store training histories for each learning rate\n",
        "histories = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"Training with learning rate: {lr}\")\n",
        "\n",
        "    # Build and compile the model with the given learning rate\n",
        "    cnn_model = build_cnn(input_shape=(224, 224, 3), x1=32, m1=3, x2=64, m2=3, x3=128, x4=64, m3=3, d=0.5, num_classes=5)\n",
        "    cnn_model.compile(optimizer=Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model and store history\n",
        "    history = cnn_model.fit(\n",
        "        train_generator,\n",
        "        epochs=10,  # Use a smaller number of epochs for quicker evaluation\n",
        "        validation_data=validation_generator,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    histories[lr] = history\n",
        "\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "for lr, history in histories.items():\n",
        "    plt.plot(history.history['loss'], label=f'Train Loss (lr={lr})')\n",
        "    plt.plot(history.history['val_loss'], linestyle='--', label=f'Val Loss (lr={lr})')\n",
        "\n",
        "plt.title('Training and Validation Loss for Different Learning Rates')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GbrK01TRnlyI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}