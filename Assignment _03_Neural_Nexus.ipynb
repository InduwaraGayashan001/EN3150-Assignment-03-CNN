{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#EN3150 Assignment 03: Simple convolutional neural network to perform classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "7eUK4ap9XmQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1 .CNN for image classification"
      ],
      "metadata": {
        "id": "SjUTjsahX56F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount the google Drive"
      ],
      "metadata": {
        "id": "VRwdhNEOs0-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov3WBfEBvHh2",
        "outputId": "f3fd9ff6-33ce-4cef-a7b1-0668341291c0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Dataset"
      ],
      "metadata": {
        "id": "vY_oerfG5pTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm  # Progress bar to monitor the copying process\n",
        "\n",
        "# Define the base directory and the new directories for train, validation, and test sets\n",
        "base_dir = '/content/drive/My Drive/EN3150-Assignment-03-CNN/Images/'\n",
        "train_dir = '/content/drive/My Drive/EN3150-Assignment-03-CNN/Images/train'\n",
        "validation_dir = '/content/drive/My Drive/EN3150-Assignment-03-CNN/Images/validation'\n",
        "test_dir = '/content/drive/My Drive/EN3150-Assignment-03-CNN/Images/test'\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Lists to hold image data and labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Load images from each folder (H1, H2, H3, H5, H6)\n",
        "for folder in os.listdir(base_dir):\n",
        "    folder_path = os.path.join(base_dir, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.endswith('.jpg'):\n",
        "                # Construct full image path\n",
        "                image_path = os.path.join(folder_path, filename)\n",
        "                image_paths.append(image_path)\n",
        "                labels.append(folder)  # Assign label based on folder name\n",
        "\n",
        "# Convert labels to a NumPy array\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "39OXoDUkXw4i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the Dataset"
      ],
      "metadata": {
        "id": "RKVleIeiYJz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data: 60% train, 20% validation, 20% test\n",
        "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "    image_paths, labels, test_size=0.4, random_state=42, stratify=labels\n",
        ")\n",
        "validation_paths, test_paths, validation_labels, test_labels = train_test_split(\n",
        "    temp_paths, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
        ")\n",
        "\n",
        "# Print the sizes of the splits\n",
        "print(f'Training set: {len(train_paths)} images')\n",
        "print(f'Validation set: {len(validation_paths)} images')\n",
        "print(f'Test set: {len(test_paths)} images')\n",
        "\n",
        "# Function to copy images to the relevant directories (train, validation, test)\n",
        "def copy_images_to_directories(image_paths, labels, split_dir):\n",
        "    # Create subdirectories for each class in the split directory (train, validation, test)\n",
        "    for class_name in np.unique(labels):\n",
        "        os.makedirs(os.path.join(split_dir, class_name), exist_ok=True)\n",
        "\n",
        "    # Use tqdm to show progress while copying files\n",
        "    for image_path, label in tqdm(zip(image_paths, labels), total=len(image_paths)):\n",
        "        # Determine the class directory for the label\n",
        "        class_dir = os.path.join(split_dir, label)\n",
        "        # Copy the image to the appropriate class directory\n",
        "        shutil.copy(image_path, class_dir)\n",
        "\n",
        "# Copy images to the train, validation, and test directories\n",
        "print(\"Copying images to the train directory...\")\n",
        "copy_images_to_directories(train_paths, train_labels, train_dir)\n",
        "\n",
        "print(\"Copying images to the validation directory...\")\n",
        "copy_images_to_directories(validation_paths, validation_labels, validation_dir)\n",
        "\n",
        "print(\"Copying images to the test directory...\")\n",
        "copy_images_to_directories(test_paths, test_labels, test_dir)\n",
        "\n",
        "print(\"Images have been copied to train, validation, and test directories.\")\n"
      ],
      "metadata": {
        "id": "s6kBEVEAW_Uf",
        "outputId": "61f2fdd0-9c81-4ab5-d2b5-ef8322f8dc81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: 5468 images\n",
            "Validation set: 1823 images\n",
            "Test set: 1823 images\n",
            "Copying images to the train directory...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5468/5468 [14:18<00:00,  6.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying images to the validation directory...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1823/1823 [04:19<00:00,  7.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying images to the test directory...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1823/1823 [04:21<00:00,  6.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images have been copied to train, validation, and test directories.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the CNN Model with modifications"
      ],
      "metadata": {
        "id": "LH5GQXquEoyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the CNN architecture\n",
        "def build_cnn(input_shape,x1, m1, x2, m2, x3, x4, m3, d, num_classes):\n",
        "    model = models.Sequential(name=\"Custom_CNN_Model\")\n",
        "\n",
        "    # Use Input layer to define the input shape\n",
        "    model.add(layers.Input(shape=input_shape, name='Input'))\n",
        "\n",
        "    # First Convolutional Layer\n",
        "    model.add(layers.Conv2D(x1, (m1, m1), activation='relu', name='Conv2D_1'))\n",
        "    model.add(layers.MaxPooling2D((2, 2), name='MaxPooling2D_1'))\n",
        "\n",
        "    # Second Convolutional Layer\n",
        "    model.add(layers.Conv2D(x2, (m2, m2), activation='relu', name='Conv2D_2'))\n",
        "    model.add(layers.MaxPooling2D((2, 2), name='MaxPooling2D_2'))\n",
        "\n",
        "    # Third Convolutional Layer\n",
        "    model.add(layers.Conv2D(x4, (m3, m3), activation='relu', name='Conv2D_3'))\n",
        "    model.add(layers.MaxPooling2D((2, 2), name='MaxPooling2D_3'))\n",
        "\n",
        "    # Fourth Convolutional layer\n",
        "    model.add(layers.Conv2D(x4, (m3, m3), activation='relu', name='Conv2D_4'))\n",
        "    model.add(layers.MaxPooling2D((2, 2), name='MaxPooling2D_4'))\n",
        "\n",
        "    # Flatten the output\n",
        "    model.add(layers.Flatten(name='Flatten'))\n",
        "\n",
        "    # Fully Connected Layer\n",
        "    model.add(layers.Dense(x3, activation='relu', name='Dense_1'))\n",
        "    model.add(layers.Dropout(d, name='Dropout_1'))  # Dropout layer to reduce overfitting\n",
        "\n",
        "    # Output Layer\n",
        "    model.add(layers.Dense(num_classes, activation='softmax', name='Output'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build and compile the CNN model\n",
        "input_shape = (224,224, 3)\n",
        "cnn_model = build_cnn(input_shape,x1=32, m1=3, x2=64, m2=3, x3=128, x4=64, m3=3, d=0.5, num_classes=5)\n",
        "\n",
        "# Custom learning rate\n",
        "custom_lr_optimizer = Adam(learning_rate=0.0001)\n",
        "\n",
        "# Using the optimizer in the model\n",
        "cnn_model.compile(optimizer=custom_lr_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "cnn_model.summary()"
      ],
      "metadata": {
        "id": "3zUPDyzSDPTd",
        "outputId": "2da5f3d6-c1f3-4c9a-ac37-7660540329ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Custom_CNN_Model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Conv2D_1 (Conv2D)           (None, 222, 222, 32)      896       \n",
            "                                                                 \n",
            " MaxPooling2D_1 (MaxPooling  (None, 111, 111, 32)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " Conv2D_2 (Conv2D)           (None, 109, 109, 64)      18496     \n",
            "                                                                 \n",
            " MaxPooling2D_2 (MaxPooling  (None, 54, 54, 64)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " Conv2D_3 (Conv2D)           (None, 52, 52, 64)        36928     \n",
            "                                                                 \n",
            " MaxPooling2D_3 (MaxPooling  (None, 26, 26, 64)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " Conv2D_4 (Conv2D)           (None, 24, 24, 64)        36928     \n",
            "                                                                 \n",
            " MaxPooling2D_4 (MaxPooling  (None, 12, 12, 64)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " Flatten (Flatten)           (None, 9216)              0         \n",
            "                                                                 \n",
            " Dense_1 (Dense)             (None, 128)               1179776   \n",
            "                                                                 \n",
            " Dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " Output (Dense)              (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1273669 (4.86 MB)\n",
            "Trainable params: 1273669 (4.86 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the Model"
      ],
      "metadata": {
        "id": "rGM6NhsoG2RU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define image size and batch size\n",
        "img_size = (224, 224)  # Resize images to 150x150\n",
        "batch_size = 32\n",
        "\n",
        "# Create ImageDataGenerators for training, validation, and test sets\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)  # Normalize images to [0, 1]\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load data from directories\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',  # Use categorical since labels are one-hot encoded\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False  # No shuffling for validation data\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = cnn_model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "save_path = '/content/drive/My Drive/models/cnn_model_1'\n",
        "cnn_model.save(save_path, save_format='tf')"
      ],
      "metadata": {
        "id": "IMljv6VxyHmk",
        "outputId": "c34d4534-4de1-4aad-c81b-f195cc4b81ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5468 images belonging to 5 classes.\n",
            "Found 1823 images belonging to 5 classes.\n",
            "Epoch 1/20\n",
            "171/171 [==============================] - 1283s 8s/step - loss: 1.2660 - accuracy: 0.5110 - val_loss: 1.2145 - val_accuracy: 0.5315\n",
            "Epoch 2/20\n",
            "171/171 [==============================] - 378s 2s/step - loss: 1.0909 - accuracy: 0.5772 - val_loss: 0.9759 - val_accuracy: 0.6072\n",
            "Epoch 3/20\n",
            "171/171 [==============================] - 374s 2s/step - loss: 1.0228 - accuracy: 0.5925 - val_loss: 0.9541 - val_accuracy: 0.6089\n",
            "Epoch 4/20\n",
            "171/171 [==============================] - 374s 2s/step - loss: 1.0041 - accuracy: 0.6020 - val_loss: 0.9190 - val_accuracy: 0.6330\n",
            "Epoch 5/20\n",
            "171/171 [==============================] - 374s 2s/step - loss: 0.9711 - accuracy: 0.6165 - val_loss: 0.9133 - val_accuracy: 0.6413\n",
            "Epoch 6/20\n",
            "171/171 [==============================] - 375s 2s/step - loss: 0.9763 - accuracy: 0.6117 - val_loss: 0.9306 - val_accuracy: 0.6341\n",
            "Epoch 7/20\n",
            "170/171 [============================>.] - ETA: 2s - loss: 0.9536 - accuracy: 0.6225"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model"
      ],
      "metadata": {
        "id": "CAsgZFTymoc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a data generator for the test dataset\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load test data\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False  # No shuffling to maintain order for evaluation\n",
        ")"
      ],
      "metadata": {
        "id": "p3uILs-ulivg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy = cnn_model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "QftjRy1nmv8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get true labels and predictions\n",
        "true_labels = test_generator.classes\n",
        "predictions = cnn_model.predict(test_generator, verbose=1)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Get class labels\n",
        "class_labels = list(test_generator.class_indices.keys())\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wRiqaP-LnBks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "report = classification_report(true_labels, predicted_labels, target_names=class_labels)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "TPVNGTrjnZdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracy = history.history['accuracy'][-1]\n",
        "val_accuracy = history.history['val_accuracy'][-1]\n",
        "\n",
        "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "jWzDVeMynhe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training and validation loss"
      ],
      "metadata": {
        "id": "seuQ6nZ8nnqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Learning rates to experiment with\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
        "\n",
        "# Dictionary to store training histories for each learning rate\n",
        "histories = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"Training with learning rate: {lr}\")\n",
        "\n",
        "    # Build and compile the model with the given learning rate\n",
        "    cnn_model = build_cnn(input_shape=(224, 224, 3), x1=32, m1=3, x2=64, m2=3, x3=128, x4=64, m3=3, d=0.5, num_classes=5)\n",
        "    cnn_model.compile(optimizer=Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model and store history\n",
        "    history = cnn_model.fit(\n",
        "        train_generator,\n",
        "        epochs=10,  # Use a smaller number of epochs for quicker evaluation\n",
        "        validation_data=validation_generator,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    histories[lr] = history\n"
      ],
      "metadata": {
        "id": "GbrK01TRnlyI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}